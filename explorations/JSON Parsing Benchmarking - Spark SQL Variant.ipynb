{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ce1813-0a50-4813-8baa-deddbc831819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Generate sample JSON data\n",
    "def generate_json(i):\n",
    "    return json.dumps({\n",
    "        \"id\": i,\n",
    "        \"user\": f\"user_{i % 100}\",\n",
    "        \"score\": random.randint(0, 100),\n",
    "        \"tags\": [f\"tag_{j}\" for j in range(i % 5)],\n",
    "        \"metadata\": {\n",
    "            \"ip\": f\"192.168.1.{i % 255}\",\n",
    "            \"device\": f\"device_{i % 10}\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "generate_json_udf = udf(generate_json, StringType())\n",
    "\n",
    "df = spark.range(0, 100_000).withColumn(\n",
    "    \"json_str\",\n",
    "    generate_json_udf(\"id\")\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ankurnayyar_cat1.demo_schema.benchmark_json_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1bb63f-9ff6-4728-aa45-adb15be47e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"score\", IntegerType()),\n",
    "    StructField(\"tags\", ArrayType(StringType())),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"ip\", StringType()),\n",
    "        StructField(\"device\", StringType())\n",
    "    ]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e8b667-1c33-4166-8a4d-806e75d68e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "df_raw = spark.table(\"ankurnayyar_cat1.demo_schema.benchmark_json_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20472731-eba3-4d30-b615-24520d047087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_json() duration: 19.20588707923889\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "df_from_json = df_raw.withColumn(\"parsed\", from_json(\"json_str\", json_schema))\n",
    "df_from_json.select(\"parsed.*\").count()\n",
    "\n",
    "print(\"from_json() duration:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3dd763-395d-40d0-81e1-a06d59e0f8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variant (from_json) duration: 5.684422254562378\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Define the schema matching your JSON structure\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"score\", IntegerType()),\n",
    "    StructField(\"tags\", ArrayType(StringType())),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"ip\", StringType()),\n",
    "        StructField(\"device\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_variant = df_raw.select(\n",
    "    from_json(\"json_str\", schema).alias(\"json_var\")\n",
    ")\n",
    "df_variant.select(\n",
    "    \"json_var.id\",\n",
    "    \"json_var.user\",\n",
    "    \"json_var.metadata.ip\"\n",
    ").count()\n",
    "\n",
    "print(\"variant (from_json) duration:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536a4192-c295-480d-a8f1-6c150df3192e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Benchmark\":231},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758567827803}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Benchmark</th><th>Method</th><th>Duration_sec</th></tr></thead><tbody><tr><td>B1_parse_from_json_count</td><td>from_json</td><td>0.924936056137085</td></tr><tr><td>B1_parse_variant_count</td><td>variant</td><td>0.7764875888824463</td></tr><tr><td>B2_select_from_json</td><td>from_json</td><td>0.6358168125152588</td></tr><tr><td>B2_select_variant</td><td>variant</td><td>0.6142821311950684</td></tr><tr><td>B3_filter_from_json</td><td>from_json</td><td>0.5060315132141113</td></tr><tr><td>B3_filter_variant</td><td>variant</td><td>0.5699584484100342</td></tr><tr><td>B4_distinct_from_json</td><td>from_json</td><td>0.6705121994018555</td></tr><tr><td>B4_distinct_variant</td><td>variant</td><td>0.6077392101287842</td></tr><tr><td>B5_group_from_json</td><td>from_json</td><td>0.7451913356781006</td></tr><tr><td>B5_group_variant</td><td>variant</td><td>0.7954986095428467</td></tr><tr><td>B6_write_from_json_delta</td><td>from_json</td><td>3.4826207160949707</td></tr><tr><td>B6_write_variant_delta</td><td>variant</td><td>2.7757928371429443</td></tr><tr><td>B7_read_from_json_delta</td><td>from_json</td><td>0.3036158084869385</td></tr><tr><td>B7_read_variant_delta</td><td>variant</td><td>0.316986083984375</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B1_parse_from_json_count",
         "from_json",
         0.924936056137085
        ],
        [
         "B1_parse_variant_count",
         "variant",
         0.7764875888824463
        ],
        [
         "B2_select_from_json",
         "from_json",
         0.6358168125152588
        ],
        [
         "B2_select_variant",
         "variant",
         0.6142821311950684
        ],
        [
         "B3_filter_from_json",
         "from_json",
         0.5060315132141113
        ],
        [
         "B3_filter_variant",
         "variant",
         0.5699584484100342
        ],
        [
         "B4_distinct_from_json",
         "from_json",
         0.6705121994018555
        ],
        [
         "B4_distinct_variant",
         "variant",
         0.6077392101287842
        ],
        [
         "B5_group_from_json",
         "from_json",
         0.7451913356781006
        ],
        [
         "B5_group_variant",
         "variant",
         0.7954986095428467
        ],
        [
         "B6_write_from_json_delta",
         "from_json",
         3.4826207160949707
        ],
        [
         "B6_write_variant_delta",
         "variant",
         2.7757928371429443
        ],
        [
         "B7_read_from_json_delta",
         "from_json",
         0.3036158084869385
        ],
        [
         "B7_read_variant_delta",
         "variant",
         0.316986083984375
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Benchmark",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Duration_sec",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewf534015\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewf534015\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewf534015\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewf534015) SELECT `Benchmark`,`Method`,SUM(`Duration_sec`) `column_9608c882675` FROM q GROUP BY `Benchmark`,`Method`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewf534015\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Method",
             "id": "column_9608c882674"
            },
            "x": {
             "column": "Benchmark",
             "id": "column_9608c882673"
            },
            "y": [
             {
              "column": "Duration_sec",
              "id": "column_9608c882675",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": "stack"
           },
           "seriesOptions": {
            "column_9608c882675": {
             "name": "Duration_sec",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "453e853f-9598-428f-a476-8a0eb4fccaaf",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Benchmark",
           "type": "column"
          },
          {
           "column": "Method",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Benchmark",
           "type": "column"
          },
          {
           "column": "Method",
           "type": "column"
          },
          {
           "alias": "column_9608c882675",
           "args": [
            {
             "column": "Duration_sec",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Benchmarking JSON Parsing vs Variant Type\n",
    "\n",
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS ankurnayyar_cat1\")\n",
    "spark.sql(\"USE CATALOG ankurnayyar_cat1\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS demo_schema\")\n",
    "spark.sql(\"USE SCHEMA demo_schema\")\n",
    "\n",
    "from pyspark.sql.functions import expr, col, from_json, udf\n",
    "from pyspark.sql.types import *\n",
    "import random, json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# UDF to generate JSON string\n",
    "def generate_json(i):\n",
    "    return json.dumps({\n",
    "        \"id\": i,\n",
    "        \"user\": f\"user_{i % 100}\",\n",
    "        \"score\": random.randint(0, 100),\n",
    "        \"tags\": [f\"tag_{j}\" for j in range(i % 5)],\n",
    "        \"metadata\": {\n",
    "            \"ip\": f\"192.168.1.{i % 255}\",\n",
    "            \"device\": f\"device_{i % 10}\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "generate_json_udf = udf(generate_json, StringType())\n",
    "\n",
    "# Create 100,000 rows of JSON data as a table\n",
    "df = spark.range(0, 100_0000).withColumn(\"json_str\", generate_json_udf(col(\"id\")))\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"json_data_raw\")\n",
    "\n",
    "# Define schema for from_json\n",
    "json_schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"user\", StringType(), nullable=True),\n",
    "    StructField(\"score\", IntegerType(), nullable=True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"metadata\", StructType([\n",
    "        StructField(\"ip\", StringType(), nullable=True),\n",
    "        StructField(\"device\", StringType(), nullable=True)\n",
    "    ]), nullable=True)\n",
    "])\n",
    "\n",
    "# Benchmark setup\n",
    "df_raw = spark.table(\"json_data_raw\")\n",
    "\n",
    "df_parsed = df_raw.withColumn(\n",
    "    \"parsed\",\n",
    "    from_json(col(\"json_str\"), json_schema)\n",
    ")\n",
    "df_variant = df_raw.withColumn(\"v\", expr(\"parse_json(json_str)\"))\n",
    "\n",
    "# Function to measure time of a lambda or function\n",
    "def measure(func, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    return result, duration\n",
    "\n",
    "# List of benchmark operations\n",
    "benchmarks = []\n",
    "\n",
    "# B1: parse JSON / variant\n",
    "_, dur = measure(lambda: df_parsed.count())\n",
    "benchmarks.append((\"B1_parse_from_json_count\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: df_variant.count())\n",
    "benchmarks.append((\"B1_parse_variant_count\", \"variant\", dur))\n",
    "\n",
    "# B2: select nested fields\n",
    "_, dur = measure(lambda: df_parsed.select(\"parsed.user\", \"parsed.metadata.device\").count())\n",
    "benchmarks.append((\"B2_select_from_json\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: df_variant.select(\n",
    "    expr(\"cast(v:user as string)\").alias(\"user\"),\n",
    "    expr(\"cast(v:metadata.device as string)\").alias(\"device\")\n",
    ").count())\n",
    "benchmarks.append((\"B2_select_variant\", \"variant\", dur))\n",
    "\n",
    "# B3: filter on nested field\n",
    "_, dur = measure(lambda: df_parsed.filter(col(\"parsed.metadata.device\") == \"device_1\").count())\n",
    "benchmarks.append((\"B3_filter_from_json\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: df_variant.filter(\n",
    "    expr(\"cast(v:metadata.device as string) = 'device_1'\")\n",
    ").count())\n",
    "benchmarks.append((\"B3_filter_variant\", \"variant\", dur))\n",
    "\n",
    "# B4: distinct count\n",
    "_, dur = measure(lambda: df_parsed.select(\"parsed.user\").distinct().count())\n",
    "benchmarks.append((\"B4_distinct_from_json\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: df_variant.select(\n",
    "    expr(\"cast(v:user as string)\").alias(\"user\")\n",
    ").distinct().count())\n",
    "benchmarks.append((\"B4_distinct_variant\", \"variant\", dur))\n",
    "\n",
    "# B5: group by nested field\n",
    "_, dur = measure(lambda: df_parsed.groupBy(\"parsed.metadata.device\").count().count())\n",
    "benchmarks.append((\"B5_group_from_json\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: df_variant.groupBy(\n",
    "    expr(\"cast(v:metadata.device as string)\").alias(\"device\")\n",
    ").count().count())\n",
    "benchmarks.append((\"B5_group_variant\", \"variant\", dur))\n",
    "\n",
    "# Writing to delta tables\n",
    "path_from_json = \"/tmp/benchmark/from_json_data\"\n",
    "path_variant = \"/tmp/benchmark/variant_data\"\n",
    "\n",
    "_, dur = measure(lambda: df_parsed.write.format(\"delta\").mode(\"overwrite\").save(path_from_json))\n",
    "benchmarks.append((\"B6_write_from_json_delta\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: df_variant.write.format(\"delta\").mode(\"overwrite\").save(path_variant))\n",
    "benchmarks.append((\"B6_write_variant_delta\", \"variant\", dur))\n",
    "\n",
    "# Reading back and projecting\n",
    "_, dur = measure(lambda: spark.read.format(\"delta\").load(path_from_json).select(\"parsed.user\").count())\n",
    "benchmarks.append((\"B7_read_from_json_delta\", \"from_json\", dur))\n",
    "\n",
    "_, dur = measure(lambda: spark.read.format(\"delta\").load(path_variant).select(\n",
    "    expr(\"cast(v:user as string)\").alias(\"user\")\n",
    ").count())\n",
    "benchmarks.append((\"B7_read_variant_delta\", \"variant\", dur))\n",
    "\n",
    "# Display results\n",
    "df_res = pd.DataFrame(benchmarks, columns=[\"Benchmark\", \"Method\", \"Duration_sec\"])\n",
    "display(df_res)\n",
    "\n",
    "# Save results as table\n",
    "spark.createDataFrame(df_res).write.mode(\"overwrite\").saveAsTable(\"benchmark_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91859f2-876d-41d8-9a25-bea8d55c051e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Benchmark</th><th>Method</th><th>Duration_sec</th><th>Memory_MB</th></tr></thead><tbody><tr><td>B1_parse_from_json_count</td><td>from_json</td><td>0.7810497283935547</td><td>0.0</td></tr><tr><td>B1_parse_variant_count</td><td>variant</td><td>0.7221431732177734</td><td>0.00390625</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B1_parse_from_json_count",
         "from_json",
         0.7810497283935547,
         0.0
        ],
        [
         "B1_parse_variant_count",
         "variant",
         0.7221431732177734,
         0.00390625
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Benchmark",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Duration_sec",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Memory_MB",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If not already installed, run in a separate cell:\n",
    "# %pip install psutil\n",
    "\n",
    "import psutil\n",
    "\n",
    "def measure_with_memory(func, *args, **kwargs):\n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss\n",
    "    start = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end = time.time()\n",
    "    mem_after = process.memory_info().rss\n",
    "    duration = end - start\n",
    "    mem_used_mb = (mem_after - mem_before) / (1024 * 1024)\n",
    "    return result, duration, mem_used_mb\n",
    "\n",
    "benchmarks = []\n",
    "\n",
    "# Example for one benchmark:\n",
    "_, dur, mem = measure_with_memory(lambda: df_parsed.count())\n",
    "benchmarks.append((\"B1_parse_from_json_count\", \"from_json\", dur, mem))\n",
    "\n",
    "# Repeat for all other benchmarks:\n",
    "_, dur, mem = measure_with_memory(lambda: df_variant.count())\n",
    "benchmarks.append((\"B1_parse_variant_count\", \"variant\", dur, mem))\n",
    "\n",
    "# ...repeat for all other operations, replacing measure() with measure_with_memory()\n",
    "\n",
    "# When done, create the DataFrame:\n",
    "df_res = pd.DataFrame(\n",
    "    benchmarks, \n",
    "    columns=[\"Benchmark\", \"Method\", \"Duration_sec\", \"Memory_MB\"]\n",
    ")\n",
    "display(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f28b87c2-47b0-41bc-a0a2-022e2b92f2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "JSON Parsing Benchmarking - Spark SQL Variant",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}