{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0824192c-f323-4818-ae41-e15d8a467177",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scenario Preparation: JSON Data Quality Test Cases"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "# Create unique path for experiment\n",
    "path = f\"/tmp/autoloader_json_quality/{int(time.time())}/\"\n",
    "input_path = path + \"input/\"\n",
    "dbutils.fs.mkdirs(input_path)\n",
    "\n",
    "# Expected schema for JSON validation\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"product\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Five JSON corruption scenarios:\n",
    "\n",
    "# 1. Invalid JSON Syntax - missing quotes and brackets (Detectable via Corrupt Record)\n",
    "invalid_syntax = '''{\"id\": 101, \"product\": {\"name\": \"Wireless Mouse\", \"category\": \"Electronics\"}, \"quantity\": 10, \"tags\": [\"wireless\", \"usb\"]}\n",
    "{id: 102, product: {name: Gaming Keyboard, category: Electronics}, quantity: 15, tags: [gaming, rgb]}'''\n",
    "\n",
    "# 2. Extra Nested Fields - additional properties beyond expected schema (Detectable via Rescue)\n",
    "extra_fields = '''{\"id\": 201, \"product\": {\"name\": \"Monitor Stand\", \"category\": \"Electronics\", \"extra_property\": \"bonus_data\"}, \"quantity\": 5, \"tags\": [\"desk\", \"ergonomic\"]}\n",
    "{\"id\": 202, \"product\": {\"name\": \"Desk Pad\", \"category\": \"Electronics\"}, \"quantity\": 8, \"tags\": [\"portable\"], \"unexpected_field\": \"additional_info\"}'''\n",
    "\n",
    "# 3. Type Mismatch in Nested Objects - wrong data types (Detectable via Rescue)\n",
    "type_mismatch = '''{\"id\": 301, \"product\": {\"name\": \"USB Hub\", \"category\": \"Electronics\"}, \"quantity\": \"N/A\", \"tags\": [\"usb\", \"hub\"]}\n",
    "{\"id\": 302, \"product\": {\"name\": \"Webcam\", \"category\": 123}, \"quantity\": 3, \"tags\": \"single-tag\"}'''\n",
    "\n",
    "# 4. Malformed Nested Arrays - invalid array syntax (Detectable via Corrupt Record)\n",
    "malformed_arrays = '''{\"id\": 401, \"product\": {\"name\": \"Power Bank\", \"category\": \"Electronics\"}, \"quantity\": 12, \"tags\": [\"portable\", \"charging\"]}\n",
    "{\"id\": 402, \"product\": {\"name\": \"Cable\", \"category\": \"Electronics\"}, \"quantity\": 20, \"tags\": [unclosed, array}'''\n",
    "\n",
    "# 5. Invalid Escape Sequences - corrupted string formatting (Detectable via Corrupt Record)\n",
    "invalid_escape = '''{\"id\": 501, \"product\": {\"name\": \"Laptop Stand\", \"category\": \"Electronics\"}, \"quantity\": 7, \"tags\": [\"laptop\", \"stand\"]}\n",
    "{\"id\": 502, \"product\": {\"name\": \"Phone Case\\\\invalid\", \"category\": \"Electronics\"}, \"quantity\": 25, \"tags\": [\"phone\", \"protection\"]}'''\n",
    "\n",
    "# Write test files\n",
    "files = {\n",
    "    \"invalid_syntax.json\": invalid_syntax,\n",
    "    \"extra_fields.json\": extra_fields,\n",
    "    \"type_mismatch.json\": type_mismatch,\n",
    "    \"malformed_arrays.json\": malformed_arrays,\n",
    "    \"invalid_escape.json\": invalid_escape\n",
    "}\n",
    "\n",
    "for filename, content in files.items():\n",
    "    dbutils.fs.put(f\"{input_path}{filename}\", content, overwrite=True)\n",
    "    \n",
    "print(f\"Created {len(files)} JSON test files covering key corruption scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed957ac-d710-4676-a8d3-ac43a5b8e0df",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_rescued_data\":622,\"file_name\":149},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755693225363}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Scenario 1: Rescue Mode with Known Schema - Nested Structure Issues"
    }
   },
   "outputs": [],
   "source": [
    "# Configure AutoLoader with known schema and rescue mode for JSON\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"json\")\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")  # Automatic nested data rescue\n",
    "  .schema(schema)  # Predefined JSON schema for validation\n",
    "  .load(input_path)\n",
    "  .select(\"*\", \"_metadata.*\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", path + \"checkpoint_1/\")\n",
    "  .trigger(availableNow=True)\n",
    "  .start(path + \"output_1/\")\n",
    "  .awaitTermination()\n",
    ")\n",
    "\n",
    "# Analyze rescued data - detects Missing Fields and Type Mismatches in nested JSON\n",
    "df_rescued = spark.read.load(path + \"output_1/\")\n",
    "rescued_records = df_rescued.filter(col(\"_rescued_data\").isNotNull())\n",
    "\n",
    "print(f\"=== SCENARIO 1: JSON RESCUE MODE WITH KNOWN SCHEMA ===\")\n",
    "print(f\"Issues detected: Extra nested fields, type mismatches in objects/arrays\")\n",
    "print(f\"Rescued records: {rescued_records.count()}\")\n",
    "\n",
    "display(rescued_records.select(\"id\", \"product.name\", \"_rescued_data\", \"file_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62103583-846c-44dc-b8d4-ed98649a446e",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"file_name\":232},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756488277926}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Scenario 2: Corrupt Record Detection - JSON Syntax Failures"
    }
   },
   "outputs": [],
   "source": [
    "# Configure AutoLoader with corrupt record detection for JSON syntax issues\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"json\")\n",
    "  .option(\"cloudFiles.schemaLocation\", path + \"schema_2/\")\n",
    "  .option(\"cloudFiles.schemaHints\", \"_corrupt_record STRING\")  # Enable JSON corruption capture\n",
    "  .load(input_path)\n",
    "  .select(\"*\", \"_metadata.*\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", path + \"checkpoint_2/\")\n",
    "  .trigger(availableNow=True)\n",
    "  .start(path + \"output_2/\")\n",
    "  .awaitTermination()\n",
    ")\n",
    "\n",
    "# Analyze corrupt records - detects Invalid Syntax, Malformed Arrays, Invalid Escapes\n",
    "df_corrupt = spark.read.load(path + \"output_2/\")\n",
    "corrupt_records = df_corrupt.filter(col(\"_corrupt_record\").isNotNull())\n",
    "\n",
    "print(f\"=== SCENARIO 2: JSON CORRUPT RECORD DETECTION ===\")\n",
    "print(f\"Issues detected: Invalid JSON syntax, malformed arrays, escape sequence errors\")\n",
    "print(f\"Corrupt records: {corrupt_records.count()}\")\n",
    "\n",
    "display(corrupt_records.select(\"_corrupt_record\", \"file_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b61a34f-9939-4058-8f13-0f38c37b64cd",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_rescued_data\":294,\"_corrupt_record\":437},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755693630945}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Scenario 3: Combined Approach - Comprehensive JSON Quality Detection"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced schema including corrupt record field for comprehensive JSON validation\n",
    "schema_with_corrupt = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"product\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Configure AutoLoader with comprehensive JSON quality detection\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"json\")\n",
    "  .schema(schema_with_corrupt)  # Complete schema with corruption handling\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")        # Nested data rescue\n",
    "  .option(\"cloudFiles.schemaHints\", \"_corrupt_record STRING\") # Syntax error capture\n",
    "  .load(input_path)\n",
    "  .select(\"*\", \"_metadata.*\")\n",
    "  .writeStream\n",
    "  .format(\"delta\")\n",
    "  .option(\"checkpointLocation\", path + \"checkpoint_3/\")\n",
    "  .trigger(availableNow=True)\n",
    "  .start(path + \"output_3/\")\n",
    "  .awaitTermination()\n",
    ")\n",
    "\n",
    "# Comprehensive JSON quality analysis - detects ALL five corruption scenarios\n",
    "df_combined = spark.read.load(path + \"output_3/\")\n",
    "all_issues = df_combined.filter(col(\"_rescued_data\").isNotNull() | col(\"_corrupt_record\").isNotNull())\n",
    "\n",
    "print(f\"=== SCENARIO 3: COMPREHENSIVE JSON QUALITY DETECTION ===\")\n",
    "print(f\"Total JSON records processed: {df_combined.count()}\")\n",
    "print(f\"Records with quality issues: {all_issues.count()}\")\n",
    "print(\"Complete JSON validation: All 5 corruption types detected\\n\")\n",
    "\n",
    "# Show detailed breakdown by issue type and source file\n",
    "print(\"--- Complete JSON Quality Analysis ---\")\n",
    "display(all_issues.select(\"id\", \"product.name\", \"_rescued_data\", \"_corrupt_record\", \"file_name\")\n",
    "        .orderBy(\"file_name\"))\n",
    "\n",
    "# JSON-specific detection summary\n",
    "rescued_count = df_combined.filter(col(\"_rescued_data\").isNotNull()).count()\n",
    "corrupt_count = df_combined.filter(col(\"_corrupt_record\").isNotNull()).count()\n",
    "print(f\"\\n✓ Extra Fields & Type Mismatches → Rescued: {rescued_count}\")\n",
    "print(f\"✓ Syntax Errors, Array/Escape Issues → Corrupt: {corrupt_count}\")\n",
    "print(f\"✓ JSON quality coverage: {((rescued_count + corrupt_count) / df_combined.count() * 100):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_f14fbde8-5353-4144-89be-f0a3e93fffc9",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "AutoLoader: JSON Data Ingestion with Built-in Data Audit & Validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
